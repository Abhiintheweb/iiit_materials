{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "\n",
    "# read training data\n",
    "diabetes_train = pd.read_csv(\"diabetes_train.csv\")\n",
    "\n",
    "# read test data\n",
    "diabetes_test = pd.read_csv(\"diabetes_test.csv\")\n",
    "\n",
    "## WRITE YOUR CODE HERE ##\n",
    "\n",
    "# Divide training data into x_train and y_train\n",
    "x_train = diabetes_train.iloc[:, :7]\n",
    "y_train = diabetes_train.iloc[:, 7]\n",
    "\n",
    "\n",
    "# Create logistic regression object log_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model=LogisticRegression()\n",
    "# Fit the model \n",
    "log_model.fit(x_train,y_train)\n",
    "\n",
    "# Make predictions\n",
    "# print(diabetes_test.head())\n",
    "predictions = log_model.predict(diabetes_test.iloc[:,:7])\n",
    "\n",
    "# manually observe the first 10 predictions\n",
    "predictions[:10]\n",
    "\n",
    "\n",
    "# Write the columns id, predictions into the output file\n",
    "# Code is already written\n",
    "# d = pd.DataFrame({'id': diabetes_test['id'], 'Diabetes_Predicted': predictions})\n",
    "# d.to_csv('/code/output/diabetes_predictions.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6242038216560509, 0.009708737864077669, 0.058823529411764705, 0.016666666666666666)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78.79"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "\n",
    "# read training data\n",
    "# read training data\n",
    "diabetes_train = pd.read_csv(\"diabetes_train.csv\")\n",
    "\n",
    "# read test data\n",
    "diabetes_test = pd.read_csv(\"diabetes_test.csv\")\n",
    "\n",
    "\n",
    "# read the file containing actual labels (for computing scores)\n",
    "actual_data = pd.read_csv(\"https://media-doselect.s3.amazonaws.com/generic/PZ2x3L59Q1k7jbRv02BJJjjZY/sample_diabetes_actual.csv\")\n",
    "\n",
    "# print(diabetes_train.head())\n",
    "# print(diabetes_test.head())\n",
    "# print(actual_data.head())\n",
    "\n",
    "## WRITE YOUR CODE HERE ##\n",
    "\n",
    "# Divide training data into x_train and y_train\n",
    "x_train = diabetes_train.iloc[:, :7]\n",
    "y_train = diabetes_train.iloc[:, 7]\n",
    "\n",
    "\n",
    "# Create logistic regression object log_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model=LogisticRegression(tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1)\n",
    "# Fit the model \n",
    "log_model.fit(x_train,y_train)\n",
    "\n",
    "# Make predictions\n",
    "# print(diabetes_test.head())\n",
    "predictions = log_model.predict_proba(diabetes_test.iloc[:,:7])\n",
    "\n",
    "# metrics.confusion_matrix\n",
    "\n",
    "\n",
    "y_predict_df=pd.DataFrame(predictions)\n",
    "y_pred_1=y_predict_df.iloc[:,[1]]\n",
    "final_data=pd.concat([actual_data,y_pred_1],1)\n",
    "final_data\n",
    "final_data=final_data.rename(columns={\"Diabetes\":\"actual\",1:\"probablity\"})\n",
    "\n",
    "final_data[\"predicted\"]=final_data.probablity.apply(lambda s:1 if s >= 0.88 else 0)\n",
    "final_data\n",
    "accuracy_score = metrics.accuracy_score(final_data.predicted,final_data.actual)\n",
    "\n",
    "confusion=metrics.confusion_matrix(final_data.predicted,final_data.actual)\n",
    "\n",
    "# Compute evaluation scores and round them off to two decimal places\n",
    "# note that the test case may fail if the metrics are not rounded off \n",
    "accuracy_score = metrics.accuracy_score(final_data.predicted,final_data.actual)\n",
    "tp=confusion[1,1]#true positive\n",
    "tn=confusion[0,0]#true negative\n",
    "fp=confusion[0,1]#false positive\n",
    "fn=confusion[1,0]#false negative\n",
    "precision_score = tp/float(tp+fp)\n",
    "recall_score = tp/float(tp+fn)\n",
    "f1_score = 2*(precision_score*recall_score/float(precision_score+recall_score))\n",
    "\n",
    "\n",
    "# m\n",
    "print(accuracy_score, precision_score, recall_score, f1_score)\n",
    "\n",
    "# write theaccuracy_score = metrics.accuracy_score(final_data.predicted,final_data.actual) scores into the output file as a dictionary\n",
    "# this code is already written for you\n",
    "# d = {'recall_score': recall_score,\n",
    "#     'f1_score': f1_score,\n",
    "#     'accuracy_score': accuracy_score,\n",
    "# \t'precision_score': precision_score}\n",
    "# print(d)\n",
    "\n",
    "# # writing the dictionary to output file\n",
    "# # code is already written\n",
    "# with open('/code/output/answers.txt', 'w') as file:\n",
    "#      file.write(json.dumps(d)) \n",
    "\n",
    "round(78.7878,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_predict_df=pd.DataFrame(predictions)\n",
    "# y_pred_1=y_predict_df.iloc[:,[1]]\n",
    "y_predict_df\n",
    "\n",
    "actual_data\n",
    "final_data=pd.concat([actual_data,y_predict_df],1)\n",
    "final_data=final_data.rename(columns={\"Diabetes\":\"actual\",0:\"predicted\"})\n",
    "final_data\n",
    "\n",
    "confusion=metrics.confusion_matrix(final_data.predicted,final_data.actual)\n",
    "\n",
    "# Compute evaluation scores and round them off to two decimal places\n",
    "# note that the test case may fail if the metrics are not rounded off \n",
    "accuracy_score = metrics.accuracy_score(final_data.predicted,final_data.actual)\n",
    "tp=confusion[1,1]#true positive\n",
    "tn=confusion[0,0]#true negative\n",
    "fp=confusion[0,1]#false positive\n",
    "fn=confusion[1,0]#false negative\n",
    "precision_score = tp/float(tp+fp)\n",
    "recall_score = tp/float(tp+fn)\n",
    "f1_score = 2*(precision_score*recall_score/float(precision_score+recall_score))\n",
    "\n",
    "\n",
    "# m\n",
    "print(accuracy_score, precision_score, recall_score, f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
